{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c38795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import gc\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b68f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()  => Standard is better here\n",
    "\n",
    "path = './BandMyo_data/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21186a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(subject):\n",
    "    lst_len = []\n",
    "    for label in range(1,10):   #1~9\n",
    "        tmp = glob.glob(path + '/00{}/00{}/*.mat'.format(subject, label))\n",
    "        for gesture in range(len(tmp)):\n",
    "            tmp2 = scipy.io.loadmat(tmp[gesture])\n",
    "            globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label, gesture)] = tmp2['data'].T\n",
    "            for i in range(8):\n",
    "                lst_len.append(len(globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label, gesture)][i]))\n",
    "\n",
    "\n",
    "    for label in range(6):  #10~15\n",
    "        tmp = glob.glob(path + '/00{}/01{}/*.mat'.format(subject, label))\n",
    "        for gesture in range(len(tmp)):\n",
    "            tmp2 = scipy.io.loadmat(tmp[gesture])\n",
    "            globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label + 10, gesture)] = tmp2['data'].T\n",
    "            for i in range(8):\n",
    "                lst_len.append(len(globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label + 10, gesture)][i]))\n",
    "\n",
    "    length = np.min(lst_len)\n",
    "    tmp_lst, labels = [], []\n",
    "                    \n",
    "\n",
    "    for label in range(1,16):\n",
    "        for gesture in range(8):\n",
    "            for idx in range(3): \n",
    "                for sensors in range(8):\n",
    "                    tm = globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label, gesture)][sensors]\n",
    "                    tmp0 = pd.DataFrame(tm)\n",
    "                    tmp = tmp0.sample(n=length)\n",
    "                    new_tmp = tmp.sort_index().values.tolist()\n",
    "                    new_tmp_ = new_tmp[200*idx:200*(idx+1)]\n",
    "                    \n",
    "                    new_tmp2 = scaler.fit_transform(new_tmp_)\n",
    "                    tmp_lst.append(list(new_tmp2.reshape(-1)))\n",
    "                    #tmp_lst.append(new_tmp_)\n",
    "                    labels.append(label-1)\n",
    "\n",
    "    for_cols = []\n",
    "    for i in range(200): for_cols.append('t{}'.format(i))\n",
    "\n",
    "    df = pd.DataFrame(tmp_lst)\n",
    "    df.columns = for_cols\n",
    "    df['Label'] = labels\n",
    "\n",
    "    #train_idx = int(len(df)/40)*30\n",
    "    #train_data = df.iloc[:train_idx, :]\n",
    "    train_data = df.sample(frac=1)\n",
    "\n",
    "    feature_names = set(df.columns) - {\"Label\"}\n",
    "    num_features = len(feature_names)\n",
    "\n",
    "    # Create train and test features as a numpy array.\n",
    "    x_train = train_data[feature_names].to_numpy()\n",
    "    y_train = train_data[\"Label\"]\n",
    "\n",
    "    node_features = tf.cast(\n",
    "        df[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    "    )\n",
    "    x_train = train_data.index.to_numpy()\n",
    "    \n",
    "    return x_train, y_train, node_features, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1b3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_build(df):\n",
    "    src, dst = [], []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        src.append(i)\n",
    "        if (i+1)%8 == 0: dst.append(i-7)\n",
    "        else: dst.append(i+1)\n",
    "        \n",
    "    graph1 = pd.DataFrame([src, dst]).T\n",
    "    graph1.columns = ['source', 'target']\n",
    "    \n",
    "    edges = graph1[[\"source\", \"target\"]].to_numpy()#.T\n",
    "    edge_weights = tf.ones(shape=edges.shape[1])\n",
    "    \n",
    "    return graph1, edges, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfd98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
    "        node_states_expanded = tf.reshape(\n",
    "            node_states_expanded, (tf.shape(edges)[0], -1)\n",
    "        )\n",
    "        attention_scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(node_states_expanded, self.kernel_attention)\n",
    "        )\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(\n",
    "            attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
    "        )\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d61db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(result):\n",
    "    loss = result.history[\"loss\"]\n",
    "    acc = result.history[\"acc\"]\n",
    "    val_loss = result.history[\"val_loss\"]\n",
    "    val_acc = result.history[\"val_acc\"]\n",
    "\n",
    "    plt.figure(figsize=(13,4))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(len(loss)),loss,label = \"Train Loss\", marker='o', markersize=3)\n",
    "    plt.plot(range(len(val_loss)),val_loss,label = \"Validation Loss\", marker='o', markersize=3)\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(len(acc)),acc,label = \"Train Accuracy\", marker='o', markersize=3)\n",
    "    plt.plot(range(len(val_acc)),val_acc,label = \"Validation Accuracy\", marker='o', markersize=3)\n",
    "    plt.title('Model Acc')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d2f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "\n",
    "        return tf.nn.softmax(tf.gather(outputs, indices))\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518a4340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_data(subject, LST_sensor):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    lst_len = []\n",
    "    for label in range(1,10):   #1~9\n",
    "        tmp = glob.glob(path + '/00{}/00{}/*.mat'.format(subject, label))\n",
    "        for gesture in range(len(tmp)):\n",
    "            tmp2 = scipy.io.loadmat(tmp[gesture])\n",
    "            globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label, gesture)] = tmp2['data'].T\n",
    "            for i in range(8):\n",
    "                lst_len.append(len(globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label, gesture)][i]))\n",
    "\n",
    "\n",
    "    for label in range(6):  #10~15\n",
    "        tmp = glob.glob(path + '/00{}/01{}/*.mat'.format(subject, label))\n",
    "        for gesture in range(len(tmp)):\n",
    "            tmp2 = scipy.io.loadmat(tmp[gesture])\n",
    "            globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label + 10, gesture)] = tmp2['data'].T\n",
    "            for i in range(8):\n",
    "                lst_len.append(len(globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label + 10, gesture)][i]))\n",
    "\n",
    "    length = np.min(lst_len)\n",
    "    tmp_lst, labels = [], []\n",
    "\n",
    "\n",
    "    for label in range(1,16):\n",
    "        for gesture in range(8):\n",
    "            for idx in range(3): \n",
    "                for sensors in LST_sensor:\n",
    "                    tm = globals()['emg_sub{}_label{}_Gesture{}'.format(subject, label, gesture)][sensors]\n",
    "                    tmp0 = pd.DataFrame(tm)\n",
    "                    tmp = tmp0.sample(n=length)\n",
    "                    new_tmp = tmp.sort_index().values.tolist()\n",
    "                    new_tmp_ = new_tmp[200*idx:200*(idx+1)]\n",
    "\n",
    "                    new_tmp2 = scaler.fit_transform(new_tmp_)\n",
    "                    tmp_lst.append(list(np.abs(new_tmp2.reshape(-1))))  ######진짜 신기한게 절대값씌워서 돌리는게 학습 더 잘됌\n",
    "                    #tmp_lst.append(list(new_tmp2.reshape(-1))) \n",
    "                    labels.append(label-1)\n",
    "\n",
    "    for_cols = []\n",
    "    for i in range(200): for_cols.append('t{}'.format(i))\n",
    "\n",
    "    df = pd.DataFrame(tmp_lst)\n",
    "    df.columns = for_cols\n",
    "    df['Label'] = labels\n",
    "    \n",
    "    train_data = df.sample(frac=1)\n",
    "\n",
    "    feature_names = set(df.columns) - {\"Label\"}\n",
    "    num_features = len(feature_names)\n",
    "\n",
    "    x_train = train_data[feature_names].to_numpy()\n",
    "    y_train = train_data[\"Label\"]\n",
    "\n",
    "    node_features = tf.cast(df[feature_names].to_numpy(), dtype=tf.dtypes.float32)\n",
    "    x_train = train_data.index.to_numpy()\n",
    "\n",
    "    return x_train, y_train, node_features, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd4c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 50\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = 15 #len(class_values)\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 128 #128\n",
    "val_split = 0.2\n",
    "LEARNING_RATE = 0.001 #0.001\n",
    "MOMENTUM = 0.9\n",
    "class_values = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbdf3bd",
   "metadata": {},
   "source": [
    "# All sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea7fee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare(lst_sensor, subject):\n",
    "    x_train, y_train, node_features, df = build_data(subject, lst_sensor)\n",
    "    graph1, edges, edge_weights = graph_build(df)\n",
    "    graph_info = (node_features, edges, edge_weights) # Create graph info tuple with node_features, edges, and edge_weights.\n",
    "    print(no_sensor, \"   Edges shape:\", edges.shape, \"    Nodes shape:\", node_features.shape)\n",
    "\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = keras.optimizers.Adam(LEARNING_RATE)\n",
    "    accuracy_fn = keras.metrics.SparseCategoricalAccuracy(name=\"acc\")\n",
    "\n",
    "    model_individual = GraphAttentionNetwork(node_features, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM)\n",
    "\n",
    "    model_individual.compile(loss=loss_fn, optimizer=optimizer, metrics=[accuracy_fn])\n",
    "    \n",
    "    return model_individual, x_train, y_train, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb71406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, random\n",
    "subject = 4\n",
    "store_path = 'C:/Users/hml76/Desktop/Jupyter/Paper1__renew/New/BandMyo_data//Subject{}/'.format(subject)\n",
    "\n",
    " \n",
    "for i in range(8): globals()['history_no_{}'.format(i)] = []\n",
    "\n",
    "all_lst = []\n",
    "for no_sensor in range(8):\n",
    "    lst_sensor = []\n",
    "    for i in range(8): \n",
    "        if i == no_sensor: pass\n",
    "        else: lst_sensor.append(i)\n",
    "    all_lst.append(lst_sensor)\n",
    "\n",
    "five_lsts = random.sample(all_lst, 5)\n",
    "\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_1.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()\n",
    "\n",
    "all_lst = []\n",
    "for no_sensor in range(8):\n",
    "    for i in range(no_sensor, 8): \n",
    "        lst_sensor = []\n",
    "        for j in range(8):\n",
    "            if i != no_sensor: \n",
    "                if j == no_sensor: pass \n",
    "                elif j == i: pass\n",
    "                else: lst_sensor.append(j)\n",
    "        \n",
    "        if lst_sensor: #lst_sensor is not empty list []\n",
    "            all_lst.append(lst_sensor)\n",
    "            \n",
    "five_lsts = random.sample(all_lst, 5)\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_2.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd003967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "7    Edges shape: (1800, 2)     Nodes shape: (1800, 200)\n",
      "Step 3\n",
      "===============================================================\n",
      "\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "7    Edges shape: (1440, 2)     Nodes shape: (1440, 200)\n",
      "Step 4\n",
      "===============================================================\n",
      "\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "7    Edges shape: (1080, 2)     Nodes shape: (1080, 200)\n",
      "Step 5\n",
      "===============================================================\n",
      "\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "7    Edges shape: (720, 2)     Nodes shape: (720, 200)\n",
      "Step 6\n",
      "===============================================================\n",
      "\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n",
      "7    Edges shape: (360, 2)     Nodes shape: (360, 200)\n"
     ]
    }
   ],
   "source": [
    "NUM_SELECT = 7\n",
    "\n",
    "all_lst = []\n",
    "for no_sensor in range(8):\n",
    "    for i in range(no_sensor, 8): \n",
    "        for k in range(i, 8):\n",
    "            lst_sensor = []\n",
    "            for j in range(8):\n",
    "                if i != no_sensor and i != k: \n",
    "                    if j == no_sensor: pass \n",
    "                    elif j == i: pass\n",
    "                    elif j == k: pass\n",
    "                    else: lst_sensor.append(j)\n",
    "        \n",
    "            if lst_sensor: #lst_sensor is not empty list []\n",
    "                all_lst.append(lst_sensor)\n",
    "\n",
    "five_lsts = random.sample(all_lst, NUM_SELECT)\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_3.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Step 3\\n===============================================================\\n\")\n",
    "\n",
    "all_lst = []\n",
    "for no_sensor in range(8):\n",
    "    for i in range(no_sensor, 8): \n",
    "        for k in range(i, 8):\n",
    "            for p in range(k, 8):\n",
    "                lst_sensor = []\n",
    "                for j in range(8):\n",
    "                    if i != no_sensor and i != k and i != p: \n",
    "                        if j == no_sensor: pass \n",
    "                        elif j == i: pass\n",
    "                        elif j == k: pass\n",
    "                        elif j == p: pass\n",
    "                        else: \n",
    "                            lst_sensor.append(j)\n",
    "                if lst_sensor and  len(lst_sensor) < 5:            \n",
    "                    all_lst.append(lst_sensor)\n",
    "\n",
    "five_lsts = random.sample(all_lst, NUM_SELECT)\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_4.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Step 4\\n===============================================================\\n\")    \n",
    "    \n",
    "all_lst = []                    \n",
    "for no_sensor in range(8):\n",
    "    for i in range(no_sensor, 8): \n",
    "        for k in range(i, 8):\n",
    "            for p in range(k, 8):\n",
    "                for q in range(p, 8):\n",
    "                    lst_sensor = []\n",
    "                    for j in range(8):\n",
    "                        if i != no_sensor and i != k and i != p and i != q: \n",
    "                            if j == no_sensor: pass \n",
    "                            elif j == i: pass\n",
    "                            elif j == k: pass\n",
    "                            elif j == p: pass\n",
    "                            elif j == q: pass\n",
    "                            else: lst_sensor.append(j)\n",
    "                                \n",
    "                    if lst_sensor and  len(lst_sensor) < 4:            \n",
    "                        all_lst.append(lst_sensor)\n",
    "\n",
    "five_lsts = random.sample(all_lst, NUM_SELECT)\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_5.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Step 5\\n===============================================================\\n\")\n",
    "\n",
    "all_lst = []\n",
    "for no_sensor in range(8):\n",
    "    for i in range(no_sensor, 8): \n",
    "        for k in range(i, 8):\n",
    "            for p in range(k, 8):\n",
    "                for q in range(p, 8):\n",
    "                    for z in range(q, 8):\n",
    "                        lst_sensor = []\n",
    "                        for j in range(8):\n",
    "                            if i != no_sensor and i != k and i != p and i != q and i != z: \n",
    "                                if j == no_sensor: pass \n",
    "                                elif j == i: pass\n",
    "                                elif j == k: pass\n",
    "                                elif j == p: pass\n",
    "                                elif j == q: pass\n",
    "                                elif j == z: pass\n",
    "                                else: lst_sensor.append(j)\n",
    "                        if lst_sensor and  len(lst_sensor) < 3:            \n",
    "                            all_lst.append(lst_sensor)\n",
    "\n",
    "five_lsts = random.sample(all_lst, NUM_SELECT)\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_6.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Step 6\\n===============================================================\\n\")\n",
    "    \n",
    "all_lst = []\n",
    "for no_sensor in range(8):\n",
    "    lst_sensor = [no_sensor]\n",
    "    all_lst.append(lst_sensor)\n",
    "    \n",
    "five_lsts = random.sample(all_lst, NUM_SELECT)\n",
    "for i in range(len(five_lsts)):\n",
    "    model_individual, x_train, y_train, optimizer = prepare(five_lsts[i], subject)\n",
    "    val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "    history_no_7.append(val)\n",
    "    del model_individual\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69877849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/hml76/Desktop/Jupyter/Paper1__renew/New/BandMyo_data//Subject4/'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac562182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    Edges shape: (2880, 2)     Nodes shape: (2880, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2226"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_individual, x_train, y_train, optimizer = prepare([0,1,2,3,4,5,6,7], subject)\n",
    "val = model_individual.fit(x_train, y_train, validation_split=val_split, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=0)\n",
    "history_no_0.append(val)\n",
    "del model_individual\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b13bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6ebf21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store_path = 'C:/Users/hml76/Desktop/Jupyter/Paper1__renew/New/performance/Subject4/'\n",
    "\n",
    "for i in range(8):\n",
    "    ACC, LOSS, VAL_ACC, VAL_LOSS = [], [], [], []\n",
    "    \n",
    "    data = globals()['history_no_{}'.format(i)]\n",
    "    for j in range(len(data)):\n",
    "        VAL_ACC.append(data[j].history['val_acc'])\n",
    "        VAL_LOSS.append(data[j].history['val_loss'])\n",
    "        ACC.append(data[j].history['acc'])\n",
    "        LOSS.append(data[j].history['loss'])\n",
    "        \n",
    "    pd.DataFrame(VAL_ACC).to_csv(store_path+'missing_{}_sensors_val_acc.csv'.format(i)) \n",
    "    pd.DataFrame(VAL_LOSS).to_csv(store_path+'missing_{}_sensors_val_loss.csv'.format(i)) \n",
    "    pd.DataFrame(ACC).to_csv(store_path+'missing_{}_sensors_training_acc.csv'.format(i)) \n",
    "    pd.DataFrame(LOSS).to_csv(store_path+'missing_{}_sensors_training_loss.csv'.format(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e12de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa15dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be087e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c435f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26346fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
